# sample-rag

# Retrieval-Augmented Generation (RAG) Architecture

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline that enables a chatbot to answer questions using your own documents as context.

---

## ğŸ”„ Architecture Flow

```mermaid
flowchart TD
    A[ğŸ’¬ User Query<br/>("Where is the Eiffel Tower?")] --> B[ğŸ§  Query Embedding<br/>(SentenceTransformer)]
    B --> C[ğŸ“š Vector Store (FAISS)<br/>Search Top-K Chunks]
    C --> D[ğŸ“‘ Retrieved Chunks<br/>("The Eiffel Tower is in Paris.")]
    D --> E[ğŸ“ Prompt Builder<br/>Context + Question]
    E --> F[ğŸ¤– LLM Generation<br/>(GPT-3.5/4 or local model)]
    F --> G[âœ… Chatbot Response<br/>"The Eiffel Tower is located in Paris, France."]

    subgraph Indexing Phase (One-time)
        H[ğŸ“‚ Document Loader<br/>PDF/Text/Web] --> I[âœ‚ï¸ Chunking<br/>Split Documents]
        I --> J[ğŸ§  Document Embedding<br/>(SentenceTransformer)]
        J --> C
    end
